{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "latest-military",
   "metadata": {},
   "source": [
    "# Machine Learning Modelling with Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-courtesy",
   "metadata": {},
   "source": [
    "# India vs Bangladesh Cricket Match and Predictive outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "limited-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source Analytics Vidya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dominant-lotus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Batsman: string (nullable = true)\n",
      " |-- Batsman_Name: string (nullable = true)\n",
      " |-- Bowler: string (nullable = true)\n",
      " |-- Bowler_Name: string (nullable = true)\n",
      " |-- Commentary: string (nullable = true)\n",
      " |-- Detail: string (nullable = true)\n",
      " |-- Dismissed: string (nullable = true)\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Isball: string (nullable = true)\n",
      " |-- Isboundary: string (nullable = true)\n",
      " |-- Iswicket: string (nullable = true)\n",
      " |-- Over: string (nullable = true)\n",
      " |-- Runs: string (nullable = true)\n",
      " |-- Timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "# create Spark Reader\n",
    "spark = SparkSession.Builder().appName('Sparky').getOrCreate()\n",
    "# read a csv file\n",
    "my_data = spark.read.csv(r'C:\\Users\\muham\\Downloads\\ind-ban-comment.csv',header=True)\n",
    "\n",
    "# see the default schema of the dataframe\n",
    "my_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "democratic-shirt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Batsman: integer (nullable = true)\n",
      " |-- Batsman_Name: string (nullable = true)\n",
      " |-- Bowler: integer (nullable = true)\n",
      " |-- Bowler_Name: string (nullable = true)\n",
      " |-- Commentary: string (nullable = true)\n",
      " |-- Detail: string (nullable = true)\n",
      " |-- Dismissed: integer (nullable = true)\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Isball: boolean (nullable = true)\n",
      " |-- Isboundary: binary (nullable = true)\n",
      " |-- Iswicket: binary (nullable = true)\n",
      " |-- Over: double (nullable = true)\n",
      " |-- Runs: integer (nullable = true)\n",
      " |-- Timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.types as tp\n",
    "\n",
    "# define the schema\n",
    "my_schema = tp.StructType([\n",
    "    tp.StructField(name= 'Batsman',      dataType= tp.IntegerType(),   nullable= True),\n",
    "    tp.StructField(name= 'Batsman_Name', dataType= tp.StringType(),    nullable= True),\n",
    "    tp.StructField(name= 'Bowler',       dataType= tp.IntegerType(),   nullable= True),\n",
    "    tp.StructField(name= 'Bowler_Name',  dataType= tp.StringType(),    nullable= True),\n",
    "    tp.StructField(name= 'Commentary',   dataType= tp.StringType(),    nullable= True),\n",
    "    tp.StructField(name= 'Detail',       dataType= tp.StringType(),    nullable= True),\n",
    "    tp.StructField(name= 'Dismissed',    dataType= tp.IntegerType(),   nullable= True),\n",
    "    tp.StructField(name= 'Id',           dataType= tp.IntegerType(),   nullable= True),\n",
    "    tp.StructField(name= 'Isball',       dataType= tp.BooleanType(),   nullable= True),\n",
    "    tp.StructField(name= 'Isboundary',   dataType= tp.BinaryType(),   nullable= True),\n",
    "    tp.StructField(name= 'Iswicket',     dataType= tp.BinaryType(),   nullable= True),\n",
    "    tp.StructField(name= 'Over',         dataType= tp.DoubleType(),    nullable= True),\n",
    "    tp.StructField(name= 'Runs',         dataType= tp.IntegerType(),   nullable= True),\n",
    "    tp.StructField(name= 'Timestamp',    dataType= tp.TimestampType(), nullable= True)    \n",
    "])\n",
    "\n",
    "# read the data again with the defined schema\n",
    "my_data = spark.read.csv(r'C:\\Users\\muham\\Downloads\\ind-ban-comment.csv',schema= my_schema,header= True)\n",
    "\n",
    "# print the schema\n",
    "my_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "scenic-kenya",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Batsman_Name',\n",
       " 'Bowler_Name',\n",
       " 'Commentary',\n",
       " 'Detail',\n",
       " 'Dismissed',\n",
       " 'Isball',\n",
       " 'Isboundary',\n",
       " 'Iswicket',\n",
       " 'Over',\n",
       " 'Runs',\n",
       " 'Timestamp']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the columns that are not required\n",
    "my_data = my_data.drop(*['Batsman', 'Bowler', 'Id'])\n",
    "my_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-hearing",
   "metadata": {},
   "source": [
    "# Using pyspark for Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "polyphonic-material",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(605, 11)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the dimensions of the data\n",
    "(my_data.count() , len(my_data.columns))\n",
    "# >> (605, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "incorrect-invasion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|              Runs|\n",
      "+-------+------------------+\n",
      "|  count|               605|\n",
      "|   mean|0.9917355371900827|\n",
      "| stddev| 1.342725481259329|\n",
      "|    min|                 0|\n",
      "|    max|                 6|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the summary of the numerical columns\n",
    "my_data.select('Isball', 'Isboundary', 'Runs').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "secondary-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sql function pyspark\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "# null values in each column\n",
    "data_agg = my_data.agg(*[f.count(f.when(f.isnull(c), c)).alias(c) for c in my_data.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "wound-alabama",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Batsman_Name: bigint, Bowler_Name: bigint, Commentary: bigint, Detail: bigint, Dismissed: bigint, Isball: bigint, Isboundary: bigint, Iswicket: bigint, Over: bigint, Runs: bigint, Timestamp: bigint]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "senior-medicare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|      Batsman_Name|count|\n",
      "+------------------+-----+\n",
      "|     Soumya Sarkar|   39|\n",
      "|  Mashrafe Mortaza|    5|\n",
      "|   Shakib Al Hasan|   75|\n",
      "|   Mushfiqur Rahim|   23|\n",
      "|Mohammad Saifuddin|   42|\n",
      "|         Liton Das|   24|\n",
      "|      Rishabh Pant|   43|\n",
      "|    Mohammed Shami|    2|\n",
      "|       Tamim Iqbal|   31|\n",
      "|     Hardik Pandya|    2|\n",
      "|          KL Rahul|   93|\n",
      "| Bhuvneshwar Kumar|    4|\n",
      "|     Rubel Hossain|   11|\n",
      "|      Rohit Sharma|   94|\n",
      "|    Dinesh Karthik|    9|\n",
      "|       Virat Kohli|   27|\n",
      "|          MS Dhoni|   33|\n",
      "|     Sabbir Rahman|   40|\n",
      "|  Mosaddek Hossain|    7|\n",
      "| Mustafizur Rahman|    1|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# value counts of Batsman_Name column\n",
    "my_data.groupBy('Batsman_Name').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-hollow",
   "metadata": {},
   "source": [
    "# Encode Categorical Variables using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "other-cooperation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+------------------+------------+\n",
      "|     Batsman_Name|Batsman_Index|       Bowler_Name|Bowler_Index|\n",
      "+-----------------+-------------+------------------+------------+\n",
      "|   Mohammed Shami|         18.0| Mustafizur Rahman|         0.0|\n",
      "|Bhuvneshwar Kumar|         16.0| Mustafizur Rahman|         0.0|\n",
      "|   Mohammed Shami|         18.0| Mustafizur Rahman|         0.0|\n",
      "|Bhuvneshwar Kumar|         16.0| Mustafizur Rahman|         0.0|\n",
      "|         MS Dhoni|          7.0| Mustafizur Rahman|         0.0|\n",
      "|         MS Dhoni|          7.0| Mustafizur Rahman|         0.0|\n",
      "|         MS Dhoni|          7.0| Mustafizur Rahman|         0.0|\n",
      "|         MS Dhoni|          7.0|Mohammad Saifuddin|         8.0|\n",
      "|         MS Dhoni|          7.0|Mohammad Saifuddin|         8.0|\n",
      "|         MS Dhoni|          7.0|Mohammad Saifuddin|         8.0|\n",
      "+-----------------+-------------+------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# String Indexing\n",
    "# String Indexing is similar to Label Encoding. \n",
    "# It assigns a unique integer value to each category.\n",
    "# 0 is assigned to the most frequent category, 1 to the next most frequent value\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "# create object of StringIndexer class and specify input and output column\n",
    "SI_batsman = StringIndexer(inputCol='Batsman_Name',outputCol='Batsman_Index')\n",
    "SI_bowler = StringIndexer(inputCol='Bowler_Name',outputCol='Bowler_Index')\n",
    "\n",
    "# transform the data\n",
    "my_data = SI_batsman.fit(my_data).transform(my_data)\n",
    "my_data = SI_bowler.fit(my_data).transform(my_data)\n",
    "\n",
    "# view the transformed data\n",
    "my_data.select('Batsman_Name', 'Batsman_Index', 'Bowler_Name', 'Bowler_Index').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-beads",
   "metadata": {},
   "source": [
    "# First, we need to use the String Indexer to convert the variable into numerical form and then use OneHotEncoderEstimator to encode multiple columns of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "deluxe-watch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+---------------+------------------+------------+--------------+\n",
      "|     Batsman_Name|Batsman_Index|    Batsman_OHE|       Bowler_Name|Bowler_Index|    Bowler_OHE|\n",
      "+-----------------+-------------+---------------+------------------+------------+--------------+\n",
      "|   Mohammed Shami|         18.0|(19,[18],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
      "|Bhuvneshwar Kumar|         16.0|(19,[16],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
      "|   Mohammed Shami|         18.0|(19,[18],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
      "|Bhuvneshwar Kumar|         16.0|(19,[16],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
      "|         MS Dhoni|          7.0| (19,[7],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
      "|         MS Dhoni|          7.0| (19,[7],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
      "|         MS Dhoni|          7.0| (19,[7],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
      "|         MS Dhoni|          7.0| (19,[7],[1.0])|Mohammad Saifuddin|         8.0|(11,[8],[1.0])|\n",
      "|         MS Dhoni|          7.0| (19,[7],[1.0])|Mohammad Saifuddin|         8.0|(11,[8],[1.0])|\n",
      "|         MS Dhoni|          7.0| (19,[7],[1.0])|Mohammad Saifuddin|         8.0|(11,[8],[1.0])|\n",
      "+-----------------+-------------+---------------+------------------+------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create object and specify input and output column\n",
    "OHE = OneHotEncoder(inputCols=['Batsman_Index', 'Bowler_Index'],outputCols=['Batsman_OHE', 'Bowler_OHE'])\n",
    "\n",
    "# transform the data\n",
    "my_data = OHE.fit(my_data).transform(my_data)\n",
    "\n",
    "# view and transform t\n",
    "my_data.select('Batsman_Name', 'Batsman_Index', 'Batsman_OHE', 'Bowler_Name', 'Bowler_Index', 'Bowler_OHE').show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-grace",
   "metadata": {},
   "source": [
    "# The Vector Assembler now converts them into a single feature column in order to train the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "damaged-northwest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              vector|\n",
      "+--------------------+\n",
      "|(34,[0,2,22,23],[...|\n",
      "|(34,[0,1,2,20,23]...|\n",
      "|(34,[0,1,2,22,23]...|\n",
      "|(34,[0,1,2,20,23]...|\n",
      "|(34,[0,2,11,23],[...|\n",
      "|(34,[0,2,11,23],[...|\n",
      "|(34,[0,2,11,23],[...|\n",
      "|(34,[0,1,2,3,11,3...|\n",
      "|(34,[0,1,2,3,11,3...|\n",
      "|(34,[0,2,3,11,31]...|\n",
      "|(34,[0,2,3,11,31]...|\n",
      "|(34,[0,1,2,3,11,3...|\n",
      "|(34,[0,1,2,3,11,3...|\n",
      "|(34,[0,2,20,23],[...|\n",
      "|(34,[0,1,2,11,23]...|\n",
      "|(34,[0,2,11,23],[...|\n",
      "|(34,[0,1,2,20,23]...|\n",
      "|(34,[0,2,17,23],[...|\n",
      "|(34,[0,1,2,11,23]...|\n",
      "|(34,[0,1,2,3,11,3...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Since ISwicket and IsBowler are not supported here, we thus remove them\n",
    "\n",
    "# specify the input and output columns of the vector assembler\n",
    "assembler = VectorAssembler(inputCols=['Over',\n",
    "                                       'Runs',\n",
    "                                       'Batsman_Index',\n",
    "                                       'Bowler_Index',\n",
    "                                       'Batsman_OHE',\n",
    "                                       'Bowler_OHE'],\n",
    "                           outputCol='vector')\n",
    "\n",
    "# fill the null values\n",
    "my_data = my_data.fillna(0)\n",
    "\n",
    "# transform the data\n",
    "final_data = assembler.transform(my_data)\n",
    "\n",
    "# view the transformed vector\n",
    "final_data.select('vector').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-milan",
   "metadata": {},
   "source": [
    "# Building Machine Learning Pipelines using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "mature-wheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A pipeline allows us to maintain the data flow of all the relevant transformations \n",
    "# that are required to reach the end result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "covered-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-serial",
   "metadata": {},
   "source": [
    "# Letâ€™s create a sample dataframe with three columns as shown below. Here, we will define some of the stages in which we want to transform the data and see how to set up the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "collaborative-monroe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+\n",
      "| id|category_1|category_2|\n",
      "+---+----------+----------+\n",
      "|  1|      L101|         R|\n",
      "|  2|      L201|         C|\n",
      "|  3|      D111|         R|\n",
      "|  4|      F210|         R|\n",
      "|  5|      D110|         C|\n",
      "+---+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define stage 3 : one hot encode the numeric category_2 column\n",
    "# create a sample dataframe\n",
    "sample_df = spark.createDataFrame([\n",
    "    (1, 'L101', 'R'),\n",
    "    (2, 'L201', 'C'),\n",
    "    (3, 'D111', 'R'),\n",
    "    (4, 'F210', 'R'),\n",
    "    (5, 'D110', 'C')\n",
    "], ['id', 'category_1', 'category_2'])\n",
    "\n",
    "sample_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "mobile-import",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----------------+----------------+--------------+\n",
      "| id|category_1|category_2|category_1_index|category_2_index|category_2_OHE|\n",
      "+---+----------+----------+----------------+----------------+--------------+\n",
      "|  1|      L101|         R|             3.0|             0.0| (1,[0],[1.0])|\n",
      "|  2|      L201|         C|             4.0|             1.0|     (1,[],[])|\n",
      "|  3|      D111|         R|             1.0|             0.0| (1,[0],[1.0])|\n",
      "|  4|      F210|         R|             2.0|             0.0| (1,[0],[1.0])|\n",
      "|  5|      D110|         C|             0.0|             1.0|     (1,[],[])|\n",
      "+---+----------+----------+----------------+----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define stage 1 : transform the column category_1 to numeric\n",
    "stage_1 = StringIndexer(inputCol= 'category_1', outputCol= 'category_1_index')\n",
    "# define stage 2 : transform the column category_2 to numeric\n",
    "stage_2 = StringIndexer(inputCol= 'category_2', outputCol= 'category_2_index')\n",
    "# define stage 3 : one hot encode the numeric category_2 column\n",
    "stage_3 = OneHotEncoder(inputCols=['category_2_index'], outputCols=['category_2_OHE'])\n",
    "\n",
    "# setup the pipeline\n",
    "pipeline = Pipeline(stages=[stage_1, stage_2, stage_3])\n",
    "\n",
    "# fit the pipeline model and transform the data as defined\n",
    "pipeline_model = pipeline.fit(sample_df)\n",
    "sample_df_updated = pipeline_model.transform(sample_df)\n",
    "\n",
    "# view the transformed data\n",
    "sample_df_updated.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "perceived-porter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+-----+\n",
      "|feature_1|feature_2|feature_3|feature_4|label|\n",
      "+---------+---------+---------+---------+-----+\n",
      "|      2.0|        A|      S10|       40|  1.0|\n",
      "|      1.0|        X|      E10|       25|  1.0|\n",
      "|      4.0|        X|      S20|       10|  0.0|\n",
      "|      3.0|        Z|      S10|       20|  0.0|\n",
      "|      4.0|        A|      E10|       30|  1.0|\n",
      "|      2.0|        Z|      S10|       40|  0.0|\n",
      "|      5.0|        X|      D10|       10|  1.0|\n",
      "+---------+---------+---------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# create a sample dataframe with 4 features and 1 label column\n",
    "sample_data_train = spark.createDataFrame([\n",
    "    (2.0, 'A', 'S10', 40, 1.0),\n",
    "    (1.0, 'X', 'E10', 25, 1.0),\n",
    "    (4.0, 'X', 'S20', 10, 0.0),\n",
    "    (3.0, 'Z', 'S10', 20, 0.0),\n",
    "    (4.0, 'A', 'E10', 30, 1.0),\n",
    "    (2.0, 'Z', 'S10', 40, 0.0),\n",
    "    (5.0, 'X', 'D10', 10, 1.0),\n",
    "], ['feature_1', 'feature_2', 'feature_3', 'feature_4', 'label'])\n",
    "\n",
    "# view the data\n",
    "sample_data_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "mediterranean-springfield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[2.0,0.0,1.0,1.0,...|  1.0|[-18.956871848873...|[5.84972108437742...|       1.0|\n",
      "|[1.0,1.0,0.0,0.0,...|  1.0|[-20.158269476976...|[1.75944137519452...|       1.0|\n",
      "|(7,[0,1,6],[4.0,1...|  0.0|[18.0148602858562...|[0.99999998499466...|       0.0|\n",
      "|(7,[0,3,6],[3.0,1...|  0.0|[24.5051237560211...|[0.99999999997721...|       0.0|\n",
      "|[4.0,0.0,1.0,0.0,...|  1.0|[-50.288624611182...|[1.44519958724377...|       1.0|\n",
      "|(7,[0,3,6],[2.0,1...|  0.0|[18.3280841853911...|[0.99999998902980...|       0.0|\n",
      "|[5.0,1.0,0.0,0.0,...|  1.0|[-17.986823547341...|[1.54319845459317...|       1.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define stage 1: transform the column feature_2 to numeric\n",
    "stage_1 = StringIndexer(inputCol= 'feature_2', outputCol= 'feature_2_index')\n",
    "# define stage 2: transform the column feature_3 to numeric\n",
    "stage_2 = StringIndexer(inputCol= 'feature_3', outputCol= 'feature_3_index')\n",
    "# define stage 3: one hot encode the numeric versions of feature 2 and 3 generated from stage 1 and stage 2\n",
    "stage_3 = OneHotEncoder(inputCols=[stage_1.getOutputCol(), stage_2.getOutputCol()], \n",
    "                                 outputCols= ['feature_2_encoded', 'feature_3_encoded'])\n",
    "# define stage 4: create a vector of all the features required to train the logistic regression model \n",
    "stage_4 = VectorAssembler(inputCols=['feature_1', 'feature_2_encoded', 'feature_3_encoded', 'feature_4'],\n",
    "                          outputCol='features')\n",
    "# define stage 5: logistic regression model                          \n",
    "stage_5 = LogisticRegression(featuresCol='features',labelCol='label')\n",
    "\n",
    "# setup the pipeline\n",
    "regression_pipeline = Pipeline(stages= [stage_1, stage_2, stage_3, stage_4, stage_5])\n",
    "\n",
    "# fit the pipeline for the trainind data\n",
    "model = regression_pipeline.fit(sample_data_train)\n",
    "# transform the data\n",
    "sample_data_train = model.transform(sample_data_train)\n",
    "\n",
    "# view some of the columns generated\n",
    "sample_data_train.select('features', 'label', 'rawPrediction', 'probability', 'prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-orange",
   "metadata": {},
   "source": [
    "# To Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "representative-exclusive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+\n",
      "|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|(7,[0,3,6],[3.0,1...|[21.9361235191363...|[0.99999999970265...|       0.0|\n",
      "|[1.0,1.0,0.0,0.0,...|[-19.516019417755...|[3.34426325212871...|       1.0|\n",
      "|(7,[0,2,6],[4.0,1...|[-22.297362790363...|[2.07194574533260...|       1.0|\n",
      "|[3.0,0.0,1.0,1.0,...|[-12.779832278243...|[2.81700837724637...|       1.0|\n",
      "|[4.0,1.0,0.0,0.0,...|[-24.163863117971...|[3.20455394170236...|       1.0|\n",
      "|(7,[0,4,6],[1.0,1...|[-22.543286459710...|[1.62022409523199...|       1.0|\n",
      "|[4.0,0.0,1.0,1.0,...|[-10.456293062940...|[2.87658445082044...|       1.0|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a sample data without the labels\n",
    "sample_data_test = spark.createDataFrame([\n",
    "    (3.0, 'Z', 'S10', 40),\n",
    "    (1.0, 'X', 'E10', 20),\n",
    "    (4.0, 'A', 'S20', 10),\n",
    "    (3.0, 'A', 'S10', 20),\n",
    "    (4.0, 'X', 'D10', 30),\n",
    "    (1.0, 'Z', 'E10', 20),\n",
    "    (4.0, 'A', 'S10', 30),\n",
    "], ['feature_1', 'feature_2', 'feature_3', 'feature_4'])\n",
    "\n",
    "# transform the data using the pipeline\n",
    "sample_data_test = model.transform(sample_data_test)\n",
    "\n",
    "# see the prediction on the test data\n",
    "sample_data_test.select('features', 'rawPrediction', 'probability', 'prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-internet",
   "metadata": {},
   "source": [
    "# Thus the Model works with Pyspark so lets test our data in the model now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "conditional-thompson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+----------+\n",
      "|       rawPrediction|probability|prediction|\n",
      "+--------------------+-----------+----------+\n",
      "|[Infinity,-Infinity]|  [1.0,0.0]|       0.0|\n",
      "+--------------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define stage 1: transform the column feature_2 to numeric\n",
    "stage_1 = StringIndexer(inputCol= 'Batsman_Name', outputCol='Batsman_Index' )\n",
    "# define stage 2: transform the column feature_3 to numeric\n",
    "stage_2 = StringIndexer(inputCol= 'Bowler_Name', outputCol= 'Bowler_Index')\n",
    "                               \n",
    "# define stage 4: create a vector of all the features required to train the logistic regression model \n",
    "stage_4 = VectorAssembler(inputCols=[ 'Batsman_Name', 'Bowler_Index'],\n",
    "                          outputCol='features')\n",
    "# define stage 5: logistic regression model                          \n",
    "stage_5 = LogisticRegression(featuresCol='features',labelCol='Bowler_Index')\n",
    "\n",
    "# setup the pipeline\n",
    "regression_pipeline = Pipeline(stages= [stage_1, stage_2, stage_4, stage_5])\n",
    "\n",
    "# fit the pipeline for the trainind data\n",
    "model = regression_pipeline.fit(data_agg)\n",
    "# transform the data\n",
    "sample_data_train = model.transform(data_agg)\n",
    "\n",
    "# view some of the columns generated\n",
    "sample_data_train.select('rawPrediction', 'probability', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-stretch",
   "metadata": {},
   "outputs": [],
   "source": [
    "Thus Our Mo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
